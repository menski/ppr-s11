\section{Grundlagen}
\label{sec:grundlagen}

Um eine Instanz der Wikipedia bereitzustellen benötigt man mehrere Komponenten. Als Basis benötigt man einen Webserver (empfohlen Apache \texttt{httpd}\footnote{\url{http://httpd.apache.org/}}), PHP\footnote{\url{http://www.php.net/}} (empfohlen $\geq$ 5.0) und MySQL\footnote{\url{http://www.mysql.com/}} (empfohlen $\geq$ 4.0). Als weitere Software wird Mediawiki\footnote{\url{http://www.mediawiki.org/}} benötigt, welches die Wikisoftware der Wikipedia ist. Nach der Installation dieser Komponenten ist ein leeres Wiki eingerichtet. Um daraus eine Instanz der Wikipedia zu erzeugen, benötigt man einen Dump der Wikipedia. Ein Dump bezeichnet hier die Sicherung der Datenbanken der Mediawiki-Installation. Dabei ist zu beachten, dass jede Sprache in der Wikipedia wiederum ein eigenes Wiki ist (z.B. \url{http://en.wikipedia.org/} oder \url{http://de.wikipedia.org/}). Somit benötigt man den Dump einer bestimmten Wikipedia, diese werden regelmäßig von der Wikimedia Foundation erstellt und zum Download angeboten\footnote{\url{http://dumps.wikimedia.org/}}. Diese Dumps enthalten nur die Sicherungen der Datenbanken und aus rechtlichen Gründen nicht die in den Wikipedia-Artikeln genutzten Bilder. Aber mit Hilfe eines Dumps und den vom Mediawiki bereitgestellten Skripten können die Wikipedia-Artikel in die Datenbank eingelesen werden. Bilder, welche in Wikipedia-Artikel eingebunden werden, werden verkleinert um Ressourcen zu sparen. Diese Verkleinerungen werden Thumbnails genannt. Die Generierung der Thumbnails erfolgt automatisch beim Aufruf eines Wikipedia-Artikels.

\subsection{WikiBench}
\label{sec:wikibench}

Die für diese Arbeit genutzten Dumps und Traces stammen von der Internetseite \url{http://www.wikibench.eu/}. Bei WikiBench handelt es sich um einen Webanwendungs-Benchmark, welcher einen Dump der englischen Wikipedia und reale Traces der Wikipedia nutzt. WikiBench wurde im Laufe einer Abschlussarbeit \cite{wikibench} an der VU Universität Amsterdam entwickelt. Die genutzten Traces wurden zudem in dem Paper \cite{wikianal} analysiert.

Der verwendete Dump enthält über 6 Millionen Artikel der englischen Wikipedia. Die Traces enthalten 25,6 Milliarden HTTP-Requests in der Zeit vom 19. September 2007 bis zum 2. Januar 2008. Jeder Request besteht aus einer eindeutigen ID, einem Zeitstempel (UNIX-Timestamp), einer URL und einem Feld, welches anzeigt ob es sich um eine Operation zum Speichern von Daten gehandelt hat. Dabei zeigt \glqq{}save\grqq{} an, dass es sich um eine Speicher-Operation gehandelt hat, ansonsten ist ein \glqq{}-\grqq{} eingetragen. Beispiele (zur besseren Darstellung teilweise gekürzt):

\begin{small}
\begin{verbatim}
5399461277 1194892944.297 http://de.wikipedia.org/wiki/Potsdam -
5[...] 1194892392.420 http://en.wikipedia.org/w/index.php?[...]&action=submit save
\end{verbatim}
\end{small}



Die Requests sind vollständig anonymisiert und entsprechen nur 10\% der tatsächlichen Requests in diesem Zeitraum. Es handelt sich nur um 10\%, da der Trace von einem der 10 Proxy-Caches der Wikimedia Foundation erzeugt wurde. Rund 43\% der Seitenanfragen richten sich an die englische Wikipedia, wobei insgesamt 32\% der Requests auf Bilder oder Thumbnails verweisen (vgl. \cite{wikianal}). Dieser Anteil zeigt, dass möglichst viele Bilder und Thumbnails bereitgestellt werden müssen, um ein sinnvolles Wiedereinspielen zu ermöglichen.

\subsection{Leistungsanalyse: Messen, Modellieren und Simulation}
\label{sec:mms}

In der Lehrveranstaltung \glqq{}Leistungsanalyse: Messen, Modellieren und Simulation\grqq{} (Sommersemester 2010) hatte Fabian Hahn, das Thema \glqq{}Server load balancing: Wikipedia\grqq{} \cite{mms}. Die Aufgabe bestand darin, den Server Load Balancer Perlbal\footnote{\url{http://www.danga.com/perlbal/}} mit Hilfe des Benchmarks httperf\footnote{\url{http://www.hpl.hp.com/research/linux/httperf/}} zu testen. Dazu nutzte er den Wikipedia-Dump und die Wikipedia-Traces von WikiBench. Die für diese Arbeit wichtigen Schlussfolgerungen aus seiner Arbeit sind, dass das Einspielen des Dumps sehr zeitintensiv und instabil ist, die Bilder nicht im Dump enthalten sind und nachgeladen werden müssen und die in den Wikipedia-Artikeln enthaltenen Thumbnails dynamisch generiert werden. Zum Nachladen der Bilder existiert im Internet ein Crawler\footnote{\url{http://meta.wikimedia.org/wiki/Wikix}}, welcher den Dump nach Bilderverweisen durchsucht und diese versucht herunterzuladen. Dabei ist zu beachten, dass er keine Thumbnails herunterlädt. Dennoch ergibt sich das Problem, dass die Thumbnails direkt in den Traces aufgerufen werden und es nicht garantiert ist, dass der entsprechende Wikipedia-Artikel vorher im Trace-Verlauf enthalten ist. Das heißt, bevor der Trace eingespielt werden kann, müssen alle Thumbnails generiert werden. Eine mögliche Strategie ist der Aufruf aller enthaltenen Wikipedia-Artikel, was aber sequentiell nicht effektiv handhabbar ist.

\subsection{Erfahrungen Praktikum Paralleles Rechnen}
\label{sec:erfahrungen}

Zu Beginn des Praktikums lautete die Aufgabenstellung für diese Arbeit:
\begin{enumerate}
\item Verteilung der Wikipedia-Instanz auf mehrere Server eines Clusters
\item Verbindung der Server durch einen Load Balancer
\item Entwicklung einer parallelen Anwendung zum Aufruf aller Wikipedia-Artikel
\end{enumerate}
Ziel dieses Vorgehens ist es, alle nötigen Thumbnails für die Traces zu generieren. Nach der erfolgreichen Installation aller Wikipedia-Instanzen und der Entwicklung einer parallelen Anwendung zum Abrufen der Wikipedia-Artikel, scheiterte dieses Vorhaben jedoch an mehreren Faktoren. In erster Linie scheiterte es an der riesigen Datenmenge, welche durch die große Anzahl an Wikipedia-Artikel und Bildern erzeugt wurde. Die zu dieser Zeit genutzte Hardware stieß dabei an ihre Speichergrenzen. Hinzu kamen Probleme bei der Thumbnail-Generierung und Hardware-Defekte. Als Lösung des Speicherplatz-Problems werden nicht die gesamten Bilder und Thumbnails verwendet, sondern nur die benötigten Ressourcen für einen Abschnitt aus den Traces. Zusätzlich werden die in diesem Trace-Abschnitt verwendeten Bilder und Thumbnails direkt von der aktuellen Wikipedia heruntergeladen. Diese veränderte Herangehensweise ergibt die im Abschnitt \ref{sec:aufgabestellung} beschriebene Aufgabenstellung.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../master"
%%% End: 

% LocalWords:  httpd PHP MySQL Mediawiki Wikisoftware Dump Dumps Wikimedia VU
% LocalWords:  Foundation WikiBench Traces Webanwendungs Paper Requests Request
% LocalWords:  ID Zeitstempel save Seitenanfragen Thumbnails vgl load balancing
% LocalWords:  Wiedereinspielen Leistungsanalyse Load Balancer Perlbal httperf
% LocalWords:  Crawler Bilderverweisen Trace Aufgabenpunkte Speichergrenzen
% LocalWords:  Thumbnail
