\section{Grundlagen}
\label{sec:grundlagen}

Um eine Instanz der Wikipedia bereitzustellen benötigt man mehrere Komponenten. Als Basis benötigt man einen Webserver (empfohlen Apache httpd\footnote{\url{http://httpd.apache.org/}}), PHP (empfohlen $\geq$ 5.0) und MySQL (empfohlen $\geq$ 4.0). Als weitere Software wird Mediawiki\footnote{\url{http://www.mediawiki.org/}} benötigt, welches die Wikisoftware der Wikipedia ist. Nach der Installation dieser Komponenten ist ein leeres Wiki eingerichtet. Um daraus eine Instanz der Wikipedia zu machen, benötigt man eine Dump der Wikipedia. Ein Dump bezeichnet hier die Sicherung der Datenbanken der Mediawiki-Installation. Dabei ist zu beachten das jede Sprache in der Wikipedia, wiederum ein eigenes Wiki ist (z.B. \url{http://en.wikipedia.org/} oder \url{http://de.wikipedia.org/}). Somit benötigt man den Dump einer bestimmten Wikipedia, diese Dumps werden regelmäßig von der Wikimedia Foundation erstellt und zum Download angeboten\footnote{\url{http://dumps.wikimedia.org/}}. Leider enthalten diese Dumps wie bereits erwähnt nur die Sicherungen der Datenbanken und aus rechtlichen Gründen nicht die in den Wikipedia-Artikeln genutzten Bilder. Aber mit Hilfe eines Dumps und den vom Mediawiki bereitgestellten Skripten können zu mindestens die Wikipedia-Artikel in die Datenbank eingelesen werden.

\subsection{WikiBench}
\label{sec:wikibench}

Die für diese Arbeit genutzten Dumps und Traces stammen von der Internetseite \url{http://www.wikibench.eu/}. Bei WikiBench handelt es sich um ein Webanwendungs-Benchmark, welcher einen Dump der englischen Wikipedia und reale Traces der Wikipedia nutzt. WikiBench wurde im Laufe einer Abschlussarbeit \cite{wikibench} an der VU Universität Amsterdam entwickelt. Die genutzten Traces wurden zudem in dem Paper \cite{wikianal} analysiert.

Der Dump enthält über 6 Millionen Artikel der englischen Wikipedia. Die Traces enthalten 25.6 Milliarden HTTP-Requests in der Zeit vom 19. September 2007 bis zum 2. Januar 2008. Jeder Request besteht aus einer eindeutigen ID, einem Zeitstempel, einer URL und einem Feld, welches anzeigt ob es sich um eine Operation zum Speichern von Daten gehandelt hat (entweder \glqq{}save\grqq{} oder \glqq{}-\grqq{}). Beispiele (zur besseren Darstellung teilweise gekürzt):

\begin{small}
\begin{verbatim}
5399461277 1194892944.297 http://de.wikipedia.org/wiki/Potsdam -
5[...] 1194892392.420 http://en.wikipedia.org/w/index.php?[...]&action=submit save
\end{verbatim}
\end{small}



Die Requests sind vollständig anonymisiert und entsprechen nur 10\% der tatsächlichen Requests in diesem Zeitraum. Rund 43\% der Seitenanfragen richten sich an die englische Wikipedia, wobei insgesamt 32\% der Requests auf Bilder oder Thumbnails verweisen (vgl. \cite{wikianal}). Dieser Anteil zeigt, dass möglichst viele Bilder und Thumbnails bereitgestellt werden müssen, um ein sinnvolles Wiedereinspielen zu ermöglichen.

\subsection{Leistungsanalyse: Messen, Modellieren und Simulation}
\label{sec:mms}

In der Lehrveranstalltung \glqq{}Leistungsanalyse: Messen, Modellieren und Simulation\grqq{} (Sommersemester 2010) hatte Fabian Hahn, das Thema \glqq{}Server load balancing: Wikipedia\grqq{} \cite{mms}. Dabei war es seine Aufgabe den Server Load Balancer Perlbal mit Hilfe des Benchmarks httperf zu testen. Dazu nutzte er den Wikipedia-Dump und die Wikipedia-Traces von WikiBench. Die für diese Arbeit wichtigen Schlussfolgerungen aus seiner Arbeit waren, dass das Einspielen des Dumps sehr zeitintensiv und instabil ist, die Bilder nicht im Dump enthalten sind und nachgeladen werden müssen und die in den Wikipedia-Artikeln enthaltenen Thumbnails dynamisch generiert werden. Zum Nachladen der Bilder existiert im Internet ein Crawler\footnote{\url{http://meta.wikimedia.org/wiki/Wikix}}, welcher den Dump nach Bilderverweisen durchsucht und diese versucht herunterzuladen. Dabei ist zu beachten, dass er keine Thumbnails herunterlädt. Trotzdem ergibt sich das Problem, dass die Thumbnails direkt in den Traces aufgerufen werden und es nicht garantiert ist, dass der entsprechende Wikipedia-Artikel vorher im Trace-Verlauf enthalten ist. Somit müssten zu erst alle Thumbnails generiert werden, bevor der Trace eingespielt werden kann. Eine simple Strategie dafür wäre ein Aufruf aller enthaltenen Wikipedia-Artikel, was aber sequentiell nicht effektiv handhabbar ist.

\subsection{Erfahrungen Praktikum Paralleles Rechnen}
\label{sec:erfahrungen}

Zu Beginn des Praktikums lauteten die Aufgabenpunkte, für diese Arbeit:
\begin{enumerate}
\item Die Wikipedia Instanz sollte auf mehrere Server eines Clusters verteilt werden.
\item Die Server sollten durch einen Load Balancer verbunden werden.
\item Es sollte eine parallele Anwendung entwickelt werden, welche alle Wikipedia-Artikel aufruft.
\end{enumerate}
Durch dieses Vorgehen sollten alle nötigen Thumbnails für die Traces generiert werden. Nach dem erfolgreichen Aufsetzen aller Wikipedia-Instanzen und der Entwicklung einer parallelen Anwendung zum Abrufen der Wikipedia-Artikel, scheiterte diese Vorhaben jedoch an mehreren Faktoren. In erster Linie scheiterte es an der riesigen Datenmenge, welche durch die große Anzahl an Wikipedia-Artikel und Bildern erzeugt wurde. Die zu dieser Zeit genutzte Hardware stieß dabei an ihre Speichergrenzen. Hinzu kamen Probleme bei der Thumbnail-Generierung und Hardware-Defekte. Als Lösung des Speicherplatz-Problems bot sich an, nicht die gesamten Bilder und Thumbnails zu verwenden, sondern nur die benötigten Ressourcen für einen Abschnitt aus den Traces. Weiterhin bot es sich an gezielt die in diesem Trace-Abschnitt verwendeten Bilder und Thumbnails direkt von der aktuellen Wikipedia herunterzuladen. Durch diese neue Herangehensweise entstand die im Abschnitt \ref{sec:aufgabestellung} beschriebene Aufgabenstellung.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../master"
%%% End: 
