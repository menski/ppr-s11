\section{Konzept}
\label{sec:konzept}

Das Konzept der hier vorgestellten Lösung besteht aus 4 Teilschritten. Als Eingabe wird ein Trace, ein Zeitabschnitt und eine Liste an zu filternden URLs benötigt.

\begin{description}
\item[1. Analyse (optional):] Der erste Schritt ist optional und dient nur der Informationsgewinnung. Hierbei werden die Request des angegeben Traces nach Host-Adressen und Bild- bzw. Thumbnail-Vorkommen analysiert. Außerdem werden optional die Requests pro Sekunde grafisch dargestellt.
\item[2. Filterung:] Im zweiten Schritt wird der gegebene Trace gefiltert. Dazu werden nur Requests weiterverarbeitet, die in dem angegeben Zeitabschnitt liegen. Außerdem werden nur Request beachtet, bei denen das letzte Feld der Requestzeile (Speicherung von Daten; siehe \ref{sec:wikibench}) ein \glqq{}-\grqq{} enthält. Also keine Speicherung von Daten durchgeführt wurde. Da es sich sonst vermutlich um \texttt{POST}-Request handelt, welche sich nicht zum Wiedereinspielen eignen, da der Request-Body fehlt. Abschließend werden nur die Requests gespeichert, welche den zu filternden URLs entsprechen. Damit können die Traces auf den existieren Dump angepasst werden. So werden in dieser konkreten Aufgabestellung nur Requests auf die englische Wikipedia und die damit verbunden Bilder-Ressourcen gefiltert.
\item[3. Download:] In diesem Schritt wird nun der gefilterte Trace ausgewertet und enthaltene Request für Bilder oder Thumbnails verarbeitet. Dazu wird zunächst geprüft, ob die entsprechende Ressource bereits in einem früheren Durchlauf heruntergeladen wurde, wenn nicht, wird versucht sie herunterzuladen. Dazu wird die im Trace angegebene URL genutzt. Ist die Ressource unter dieser URL nicht mehr verfügbar, wurde sie seit Erstellung des Traces verschoben bzw. umbenannt oder gelöscht. Dann ist es nicht möglich, diese Ressource lokal bereitzustellen. Nachdem alle verfügbaren Ressourcen heruntergeladen sind, müssen die Bilder der Wikipedia Datenbank bekannt gemacht werden. Dafür existiert ein PHP-Skript im Mediawiki, welches Metadaten der Bilder in der Datenbank abspeichert. Dieses Skript ist sehr langsam und speicherintensiv, allerdings existiert keine Alternative.
\item[4. Installation:] Nachdem alle Ressourcen heruntergeladen sind und die Datenbank aktualisiert ist, können diese auf die anderen Server im Cluster transferiert und dort lokal eingerichtet werden. Anschließend sind alle Server des Clusters mit der gleichen Wikipedia Instanz ausgestattet.
\end{description}
Bis auf Schritt 1 sind alle Schritte erforderlich und benötigen mindestens einen Durchlauf des vorherigen Schrittes. Das heißt Schritt 3 kann erst ausgeführt werden, wenn Schritt 2 mindestens einmal ausgeführt wurde. Dann kann Schritt 3 aber beliebig wiederholt werden, ohne das Schritt 2 noch einmal ausgeführt werden muss. Das gleiche gilt für Schritt 4 bezüglich Schritt 3.

Ein weiterer Punkt des Konzepts ist es, alle Schritte in einer Anwendung zu vereinen und diese Anwendung über eine Konfigurationsdatei zu steuern. Die Verwendung von Kommandozeilenoptionen schien nicht ratsam auf Grund der Fülle an Einstellungsmöglichkeiten und benötigten Parametern. Des Weiteren, ist es ratsam, die verfügbare CPU, möglichst effizient auszunutzen, weshalb eine Parallelisierung der Anwendung sinnvoll ist.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../master"
%%% End: 

% LocalWords:  Teilschritten Trace Request Traces Host bzw Thumbnail Requests
% LocalWords:  Requestzeile Wiedereinspielen Body Dump Thumbnails PHP Mediawiki
% LocalWords:  Metadaten Kommandozeilenoptionen Einstellungsmöglichkeiten
