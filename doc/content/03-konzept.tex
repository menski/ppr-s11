\section{Konzept}
\label{sec:konzept}

Das Konzept der hier vorgestellten Lösung besteht aus 4 Teilschritten. Als Eingabe wird ein Trace, ein Zeitabschnitt und eine Liste an zu filternden URLs benötigt.

\begin{description}
\item[1. Analyse (optional):] Der erste Schritt ist optional und dient nur der Informationsgewinnung. Hierbei werden die Request des angegeben Traces nach Host-Adressen und Bild- bzw. Thumbnail-Vorkommen analysiert. Außerdem werden optional die Requests pro Sekunde grafisch dargestellt.
\item[2. Filterung:] Im zweiten Schritt, wird der gegeben Trace gefiltert. Dazu werden nur Requests weiterverarbeitet, die in dem angegeben Zeitabschnitt liegen. Außerdem werden nur Request beachtet bei denen das letzte Feld der Requestzeile (Speicherung von Daten; siehe \ref{sec:wikibench}) ein \glqq{}-\grqq{} enthält. Also keine Speicherung von Daten durchgeführt wurde. Da es sich sonst vermutlich um \texttt{POST} Request handelte, welche sich nicht zum Wiedereinspielen eignen, da der Request-Body fehlt. Abschließend werden nur die Requests gespeichert, welche den zu filternden URLs entsprechen. Damit können die Traces auf den existieren Dump angepasst werden. So werden in diesem konkreten Beispiel nur Requests auf die englische Wikipedia und die damit verbunden Bilder-Ressourcen gefiltert.
\item[3. Download:] In diesem Schritt wird nun der gefilterte Trace ausgewertet und enthaltene Request für Bilder oder Thumbnails verarbeitet. Dazu wird zuerst geprüft ob die entsprechende Ressource bereits in einem früheren Durchlauf heruntergeladen wurden, wenn nicht wird versucht sie herunterzuladen. Dazu wird die im Trace angegebene URL genutzt. Ist die Ressource unter dieser URL nicht mehr verfügbar wurde sie seit Erstellung des Traces, entweder verschoben bzw. umbenannt oder gelöscht. Leider ist es dann nicht möglich, diese Ressource lokal bereitzustellen. Nachdem alle verfügbaren Ressourcen heruntergeladen wurden, müssen die Bilder der Wikipedia Datenbank bekannt gemacht werden. Dafür existiert ein PHP Skript im Mediawiki, welches Metadaten der Bilder in der Datenbank abspeichert. Dieses Skript ist sehr langsam und speicherintensiv, allerdings existiert leider keine Alternative.
\item[4. Installation:] Nachdem alle Ressourcen heruntergeladen wurden und die Datenbank aktualisiert wurde, können diese auf die anderen Server im Cluster transferiert werden und dort lokal eingerichtet werden. Anschließend sind alle Server des Clusters mit der selben Wikipedia Instanz ausgestattet.
\end{description}
Bis auf Schritt 1 sind alle Schritte erforderlich und benötigen mindestens einen Durchlauf des vorherigen Schrittes. Das heißt Schritt 3 kann erst ausgeführt werden, wenn Schritt 2 mindestens einmal ausgeführt wurde. Dann kann Schritt 3 aber beliebig wiederholt werden, ohne das Schritt 2 noch einmal ausgeführt werden muss. Das gleiche gilt für Schritt 4 bezüglich Schritt 3.

Ein weitere Punkt des Konzepts ist es, alle Schritte in einer Anwendung zu vereinen und diese Anwendung über eine Konfigurationsdatei zu steuern. Die Verwendung von Kommandozeilenoptionen schien nicht ratsam auf Grund der Fülle an Einstellungsmöglichkeiten und benötigten Parametern. Des weiteren, sollte die verfügbare CPU möglichst effizient ausgenutzt werden, weshalb eine Parallelisierung der Anwendung sinnvoll ist.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../master"
%%% End: 
